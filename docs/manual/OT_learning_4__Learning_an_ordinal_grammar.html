<html><head><meta name="robots" content="index,follow"><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>OT learning 4. Learning an ordinal grammar</title>
<style>
   td { padding-left: 5pt; padding-right: 5pt; }
   th { padding-left: 5pt; padding-right: 5pt; }
   code { white-space: pre-wrap; }
   dd { white-space: pre-wrap; }
</style>
</head><body bgcolor="#FFFFFF">

<table border=0 cellpadding=0 cellspacing=0><tr><td bgcolor="#CCCC00"><table border=4 cellpadding=9><tr><td align=middle bgcolor="#000000"><font face="Palatino,Times" size=6 color="#999900"><b>
OT learning 4. Learning an ordinal grammar
</b></font></table></table>
<p>With the data from a tongue-root-harmony language with five completely ranked constraints, we can have a throw at learning this language, starting with a grammar in which all the constraints are ranked at the same height, or randomly ranked, or with articulatory constraints outranking faithfulness constraints.</p>
<p>Let&#8217;s try the third of these. Create an infant tongue-root grammar by choosing <a href="Create_tongue-root_grammar___.html">Create tongue-root grammar...</a> and specifying &#8220;Five&#8221; for the constraint set and &#8220;Infant&#8221; for the ranking. The result after a single evaluation will be like:</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      100.000<td width=100 align=middle>      100.631<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      100.000<td width=100 align=middle>      100.244<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      100.000<td width=100 align=middle>      97.086<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      50.000<td width=100 align=middle>      51.736<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      50.000<td width=100 align=middle>      46.959<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>Such a grammar produces all kinds of non-adult results. For instance, the input /&#601;t&#618;/ will surface as [at&#618;]:</p>
<p align=middle><img height=150 width=400 src=OT_learning_4__Learning_an_ordinal_grammar_1.png></p><p>The adult form is very different: [&#601;ti]. The cause of the discrepancy is in the order of the constraints *[atr / lo] and *[rtr / hi], which militate against [&#601;] and [&#618;], respectively. Simply reversing the rankings of these two constraints would solve the problem in this case. More generally, <a href="Tesar___Smolensky__1998_.html">Tesar & Smolensky (1998)</a> claim that demoting all the constraints that cause the adult form to lose into the stratum just below the highest-ranked constraint violated in the learner's form (here, moving *[atr / lo] just below *[rtr / hi] into the same stratum as P<font size=-1>ARSE</font> (rtr)), will guarantee convergence to the target grammar, <i>if there is no variation in the data</i> (Tesar &amp; Smolensky's algorithm is actually incorrect, but can be repaired easily, as shown by <a href="Boersma__2009b_.html">Boersma (2009b)</a>).</p>
<p>But Tesar &amp; Smolensky's algorithm cannot be used for variable data, since all constraints would be tumbling down, exchanging places and producing wildly different grammars at each learning step. Since language data do tend to be variable, we need a gradual and balanced learning algorithm, and the following algorithm is guaranteed to converge to the target language, if that language can be described by a stochastic OT grammar.</p>
<p>The reaction of the learner to hearing the mismatch between the adult [&#601;ti] and her own [at&#618;], is simply:</p>
<dl>
<dd style="position:relative;padding-left:1em;text-indent:-2em">1. to move the constraints violated in her own form, i.e. *[rtr / hi] and P<font size=-1>ARSE</font> (atr), up by a small step along the ranking scale, thus decreasing the probability that her form will be the winner at the next evaluation of the same input;</dd>
<dd style="position:relative;padding-left:1em;text-indent:-2em">2. and to move the constraints violated in the adult form, namely *[atr / lo] and P<font size=-1>ARSE</font> (rtr), down along the ranking scale, thus increasing the probability that the adult form will be the learner's winner the next time.</dd>
</dl>
<p>If the small reranking step (the <i><b>plasticity</i></b>) is 0.1, the grammar will become:</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      100.000<td width=100 align=middle>      100.631<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      99.900<td width=100 align=middle>      100.244<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      100.100<td width=100 align=middle>      97.086<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      49.900<td width=100 align=middle>      51.736<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      50.100<td width=100 align=middle>      46.959<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>The disharmonies, of course, will be different at the next evaluation, with a probability slightly higher than 50% that *[rtr / hi] will outrank *[atr / lo]. Thus the relative rankings of these two grounding constraints have moved into the direction of the adult grammar, in which they are ranked at opposite ends of the grammar.</p>
<p>Note that the relative rankings of P<font size=-1>ARSE</font> (atr) and P<font size=-1>ARSE</font> (rtr) are now moving in a direction opposite to where they will have to end up in this RTR-dominant language. This does not matter: the procedure will converge nevertheless.</p>
<p>We are now going to simulate the infant who learns simplified Wolof. Take an adult Wolof grammar and generate 1000 input strings and the corresponding 1000 output strings following the procedure described in <a href="OT_learning_3_2__Data_from_another_grammar.html">&#167;3.2</a>. Now select the infant <a href="OTGrammar.html">OTGrammar</a> and both <a href="Strings.html">Strings</a> objects, and choose <a href="OTGrammar___2_Strings__Learn___.html">Learn...</a>. After you click <b>OK</b>, the learner processes each of the 1000 input-output pairs in succession, gradually changing the constraint ranking in case of a mismatch. The resulting grammar may look like:</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      100.800<td width=100 align=middle>      98.644<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      89.728<td width=100 align=middle>      94.774<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      89.544<td width=100 align=middle>      86.442<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      66.123<td width=100 align=middle>      65.010<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      63.553<td width=100 align=middle>      64.622<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>We already see some features of the target grammar, namely the top ranking of *[rtr / hi] and RTR dominance (the mutual ranking of the P<font size=-1>ARSE</font> constraints). The steps have not been exactly 0.1, because we also specified a relative plasticity spreading of 0.1, thus giving steps typically in the range of 0.7 to 1.3. The step is also multiplied by the <i>constraint plasticity</i>, which is simply 1.000 in all examples in this tutorial; you could set it to 0.0 to prevent a constraint from moving up or down at all. The <i>leak</i> is the part of the constraint weight (especially in Harmonic Grammar) that is thrown away whenever a constraint is reranked; e.g if the leak is 0.01 and the step is 0.11, the constraint weight is multiplied by (1 &#8211; 0.01&#183;0.11) = 0.9989 before the learning step is taken; in this way you could implement forgetful learning of correlations.</p>
<p>After learning once more with the same data, the result is:</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      100.800<td width=100 align=middle>      104.320<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      81.429<td width=100 align=middle>      82.684<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      79.966<td width=100 align=middle>      78.764<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      81.316<td width=100 align=middle>      78.166<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      77.991<td width=100 align=middle>      77.875<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>This grammar now sometimes produces faithful disharmonic utterances, because the P<font size=-1>ARSE</font> now often outrank the gestural constraints at evaluation time. But there is still a lot of variation produced. Learning once more with the same data gives:</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      100.800<td width=100 align=middle>      100.835<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      86.392<td width=100 align=middle>      82.937<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      81.855<td width=100 align=middle>      81.018<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      78.447<td width=100 align=middle>      78.457<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      79.409<td width=100 align=middle>      76.853<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>By inspecting the first column, you can see that the ranking values are already in the same order as in the target grammar, so that the learner will produce 100 percent correct adult utterances if her evaluation noise is zero. However, with a noise of 2.0, there will still be variation. For instance, the disharmonies above will produce [ata] instead of [&#601;t&#601;] for underlying /&#601;t&#601;/. Learning seven times more with the same data gives a reasonable proficiency:</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      100.800<td width=100 align=middle>      99.167<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      91.580<td width=100 align=middle>      93.388<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      85.487<td width=100 align=middle>      86.925<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      80.369<td width=100 align=middle>      78.290<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      75.407<td width=100 align=middle>      74.594<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>No input forms have error rates above 4 percent now, so the child has learned a lot with only 10,000 data, which may be on the order of the number of input data she receives every day.</p>
<p>We could have sped up the learning process appreciably by using a plasticity of 1.0 instead of 0.1. This would have given a comparable grammar after only 1000 data. After 10,000 data, we would have</p>
<dl>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><td width=100 align=middle>      <i>ranking value<td width=100 align=middle>      </i>disharmony<td width=100 align=middle>      <i>plasticity</i></table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[rtr / hi]</b><td width=100 align=middle>      107.013<td width=100 align=middle>      104.362<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (rtr)</b><td width=100 align=middle>      97.924<td width=100 align=middle>      99.984<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*G<font size=-1>ESTURE</font> (contour)</b><td width=100 align=middle>      89.679<td width=100 align=middle>      89.473<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>P<font size=-1>ARSE</font> (atr)</b><td width=100 align=middle>      81.479<td width=100 align=middle>      83.510<td width=100 align=middle>       1.000</table></dd>
<dd>&nbsp;&nbsp;&nbsp;<table border=0 cellpadding=0 cellspacing=0><tr><td width=100 align=middle><b>*[atr / lo]</b><td width=100 align=middle>      73.067<td width=100 align=middle>      72.633<td width=100 align=middle>       1.000</table></dd>
</dl>
<p>With this grammar, all the error rates are below 0.2 percent. We see that crucially ranked constraints will become separated after a while by a gap of about 10 along the ranking scale.</p>
<p>If we have three constraints obligatorily ranked as A &gt;&gt; B &gt;&gt; C in the adult grammar, with ranking differences of 8 between A and B and between B and C in the learner's grammar (giving an error rate of 0.2%), the ranking A &gt;&gt; C has a chance of less than 1 in 100 million to be reversed at evaluation time. This relativity of error rates is an empirical prediction of our stochastic OT grammar model.</p>
<p>Our Harmonic Grammars with constraint noise (Noisy HG) are slightly different in that respect, but are capable of learning a constraint ranking for any language that can be generated from an ordinal ranking. As proved by <a href="Boersma___Pater__2016_.html">Boersma & Pater (2016)</a>, the same learning rule as was devised for MaxEnt grammars by <a href="Jäger__2003_.html">Jäger (2003)</a> is able to learn all languages generated by <i>nonnoisy</i> HG grammars as well; the GLA, by contrast, failed to converge on 0.4 percent of randomly generated OT languages (failures of the GLA on ordinal grammars were discovered first by <a href="Pater__2008_.html">Pater (2008)</a>). This learning rule for HG and MaxEnt is the same as the GLA described above, except that the learning step of each constraint is multiplied by the difference of the number of violations of this constraint between the correct form and the incorrect winner; this multiplication is crucial (without it, stochastic gradient ascent is not guaranteed to converge), as was noted by J&#228;ger for MaxEnt. The same procedure for updating weights occurs in <a href="Soderstrom__Mathis___Smolensky__2006_.html">Soderstrom, Mathis & Smolensky (2006)</a>, who propose an incremental version (formulas 21 and 35d) of the harmony version (formulas 14 and 18) of the learning equation for Boltzmann machines (formula 13). The differences between the three implementations is that in Stochastic OT and Noisy HG the evaluation noise (or <i>temperature</i>) is in the constraint rankings, in MaxEnt it is in the candidate probabilities, and in Boltzmann machines it is in the activities (i.e. the constraint violations). The upate procedure is also similar to that of the <i>perceptron</i>, a neural network invented by <a href="Rosenblatt__1962_.html">Rosenblatt (1962)</a> for classifying continuous inputs.</p>
<h3>Links to this page</h3>
<ul>
<li><a href="OT_learning.html">OT learning</a>
<li><a href="OTGrammar__Learn_one___.html">OTGrammar: Learn one...</a>
</ul>
<hr>
<address>
	<p>© ppgb 20190331</p>
</address>
</body>
</html>
